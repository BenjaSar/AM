{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bpJ7s_SIVu_I"
      },
      "source": [
        "# Trabajo Práctico Final: Linear/Quadratic Discriminant Analysis (LDA/QDA)\n",
        "\n",
        "### Definición: Clasificador Bayesiano\n",
        "\n",
        "Sean $k$ poblaciones, $x \\in \\mathbb{R}^p$ puede pertenecer a cualquiera $g \\in \\mathcal{G}$ de ellas. Bajo un esquema bayesiano, se define entonces $\\pi_j \\doteq P(G = j)$ la probabilidad *a priori* de que $X$ pertenezca a la clase *j*, y se **asume conocida** la distribución condicional de cada observable dado su clase $f_j \\doteq f_{X|G=j}$.\n",
        "\n",
        "De esta manera dicha probabilidad *a posteriori* resulta\n",
        "$$\n",
        "P(G = j|X=x) = \\frac{P(X=x|G = j) P(G = j)}{P(X=x)} = \\frac{f_{X|G=j}(x) \\cdot p_G(j)}{f_X(x)} \\propto f_j(x) \\cdot \\pi_j\n",
        "$$\n",
        "\n",
        "$$\n",
        "P(G|_{X=x} = j) = \\frac{f_{X|G=j}(x) \\cdot p_G(j)}{f_X(x)} \\propto f_j(x) \\cdot \\pi_j\n",
        "$$\n",
        "\n",
        "La regla de decisión de Bayes es entonces\n",
        "$$\n",
        "H(x) \\doteq \\arg \\max_{g \\in \\mathcal{G}} \\{ P(G = g | X=x) \\} = \\arg \\max_{g \\in \\mathcal{G}} \\{ f_g(x) \\cdot \\pi_g \\}\n",
        "$$\n",
        "\n",
        " \n",
        "\n",
        "es decir, se predice a $x$ como perteneciente a la población $j$ cuya probabilidad a posteriori es máxima.\n",
        "\n",
        "*Ojo, a no desesperar! $\\pi_j$ no es otra cosa que una constante prefijada, y $f_j$ es, en su esencia, un campo escalar de $x$ a simplemente evaluar.*\n",
        "\n",
        "### Distribución condicional\n",
        "\n",
        "Para los clasificadores de discriminante cuadrático y lineal (QDA/LDA) se asume que $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma_j)$, es decir, se asume que cada población sigue una distribución normal.\n",
        "\n",
        "Por definición, se tiene entonces que para una clase $j$:\n",
        "$$\n",
        "f_j(x) = \\frac{1}{(2 \\pi)^\\frac{p}{2} \\cdot |\\Sigma_j|^\\frac{1}{2}} e^{- \\frac{1}{2}(x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j)}\n",
        "$$\n",
        "\n",
        "Aplicando logaritmo (que al ser una función estrictamente creciente no afecta el cálculo de máximos/mínimos), queda algo mucho más práctico de trabajar:\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "\n",
        "Observar que en este caso $C=-\\frac{p}{2} \\log(2\\pi)$, pero no se tiene en cuenta ya que al tener una constante aditiva en todas las clases, no afecta al cálculo del máximo.\n",
        "\n",
        "### LDA\n",
        "\n",
        "En el caso de LDA se hace una suposición extra, que es $X|_{G=j} \\sim \\mathcal{N}_p(\\mu_j, \\Sigma)$, es decir que las poblaciones no sólo siguen una distribución normal sino que son de igual matriz de covarianzas. Reemplazando arriba se obtiene entonces:\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  -\\frac{1}{2}\\log |\\Sigma| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "Ahora, como $-\\frac{1}{2}\\log |\\Sigma|$ es común a todas las clases se puede incorporar a la constante aditiva y, distribuyendo y reagrupando términos sobre $(x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j)$ se obtiene finalmente:\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n",
        "$$\n",
        "\n",
        "### Entrenamiento/Ajuste\n",
        "\n",
        "Obsérvese que para ambos modelos, ajustarlos a los datos implica estimar los parámetros $(\\mu_j, \\Sigma_j) \\; \\forall j = 1, \\dots, k$ en el caso de QDA, y $(\\mu_j, \\Sigma)$ para LDA.\n",
        "\n",
        "Estos parámetros se estiman por máxima verosimilitud, de manera que los estimadores resultan:\n",
        "\n",
        "* $\\hat{\\mu}_j = \\bar{x}_j$ el promedio de los $x$ de la clase *j*\n",
        "* $\\hat{\\Sigma}_j = s^2_j$ la matriz de covarianzas estimada para cada clase *j*\n",
        "* $\\hat{\\pi}_j = f_{R_j} = \\frac{n_j}{n}$ la frecuencia relativa de la clase *j* en la muestra\n",
        "* $\\hat{\\Sigma} = \\frac{1}{n} \\sum_{j=1}^k n_j \\cdot s^2_j$ el promedio ponderado (por frecs. relativas) de las matrices de covarianzas de todas las clases. *Observar que se utiliza el estimador de MV y no el insesgado*\n",
        "\n",
        "Es importante notar que si bien todos los $\\mu, \\Sigma$ deben ser estimados, la distribución *a priori* puede no inferirse de los datos sino asumirse previamente, utilizándose como entrada del modelo.\n",
        "\n",
        "### Predicción\n",
        "\n",
        "Para estos modelos, al igual que para cualquier clasificador Bayesiano del tipo antes visto, la estimación de la clase es por método *plug-in* sobre la regla de decisión $H(x)$, es decir devolver la clase que maximiza $\\hat{f}_j(x) \\cdot \\hat{\\pi}_j$, o lo que es lo mismo $\\log\\hat{f}_j(x) + \\log\\hat{\\pi}_j$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TDWOgpJWKQa"
      },
      "source": [
        "## Estructura del código"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yEV8WbiWl6k"
      },
      "source": [
        "## Modelo "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "teF9O9JJmG7Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import det, inv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sDBLvbTtlwzs"
      },
      "outputs": [],
      "source": [
        "class ClassEncoder:\n",
        "  def fit(self, y):\n",
        "    self.names = np.unique(y)\n",
        "    self.name_to_class = {name:idx for idx, name in enumerate(self.names)}\n",
        "    self.fmt = y.dtype\n",
        "    # Q1: why is there no need for class_to_name?\n",
        "    # el proceso de decodificación se realiza mediante \n",
        "    # la matriz self.names que se indexa utilizando los índices de clase directamente.\n",
        "\n",
        "  def _map_reshape(self, f, arr):\n",
        "    return np.array([f(elem) for elem in arr.flatten()]).reshape(arr.shape)\n",
        "    # Q2: why is reshaping necessary?\n",
        "    # esto garantiza que la matriz de salida tenga la misma forma que la matriz de entrada.\n",
        "\n",
        "  def transform(self, y):\n",
        "    return self._map_reshape(lambda name: self.name_to_class[name], y)\n",
        "\n",
        "  def fit_transform(self, y):\n",
        "    self.fit(y)\n",
        "    return self.transform(y)\n",
        "\n",
        "  def detransform(self, y_hat):\n",
        "    return self._map_reshape(lambda idx: self.names[idx], y_hat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "m0KYC8_uSOu4"
      },
      "outputs": [],
      "source": [
        "class BaseBayesianClassifier:\n",
        "  def __init__(self):\n",
        "    self.encoder = ClassEncoder()\n",
        "\n",
        "  def _estimate_a_priori(self, y):\n",
        "    a_priori = np.bincount(y.flatten().astype(int)) / y.size\n",
        "    # Q3: what does bincount do?\n",
        "    return np.log(a_priori)\n",
        "    # Segun la documentación de numpy: cuenta las ocurrencias de números enteros no negativos en un array.\n",
        "    # O sea se utiliza para contar las ocurrencias de cada clase en y.\n",
        "    \n",
        "  def _fit_params(self, X, y):\n",
        "    # estimate all needed parameters for given model\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
        "    # this should depend on the model used\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def fit(self, X, y, a_priori=None):\n",
        "    # first encode the classes\n",
        "    y = self.encoder.fit_transform(y)\n",
        "\n",
        "    # if it's needed, estimate a priori probabilities\n",
        "    self.log_a_priori = self._estimate_a_priori(y) if a_priori is None else np.log(a_priori)\n",
        "\n",
        "    # check that a_priori has the correct number of classes\n",
        "    assert len(self.log_a_priori) == len(self.encoder.names), \"A priori probabilities do not match number of classes\"\n",
        "\n",
        "    # now that everything else is in place, estimate all needed parameters for given model\n",
        "    self._fit_params(X, y)\n",
        "    # Q4: why do we need to do this last, can't we do it first?\n",
        "    # No, porque antes de de estimar los parámetros del modelo, hay que\n",
        "    # codificar las clases y calcular las probabilidades a priori.\n",
        "\n",
        "  def predict(self, X):\n",
        "    # this is actually an individual prediction encased in a for-loop\n",
        "    m_obs = X.shape[1]\n",
        "    y_hat = np.empty(m_obs, dtype=self.encoder.fmt)\n",
        "    \n",
        "    for i in range(m_obs):\n",
        "      encoded_y_hat_i = self._predict_one(X[:,i].reshape(-1,1))\n",
        "      y_hat[i] = self.encoder.names[encoded_y_hat_i]\n",
        "\n",
        "    # return prediction as a row vector (matching y)\n",
        "    return y_hat.reshape(1,-1)\n",
        "\n",
        "  def _predict_one(self, x):\n",
        "    # calculate all log posteriori probabilities (actually, +C)\n",
        "    log_posteriori = [ log_a_priori_i + self._predict_log_conditional(x, idx) for idx, log_a_priori_i \n",
        "                  in enumerate(self.log_a_priori) ]\n",
        "\n",
        "    # return the class that has maximum a posteriori probability\n",
        "    return np.argmax(log_posteriori)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IRamFdiGDuSR"
      },
      "outputs": [],
      "source": [
        "class QDA(BaseBayesianClassifier):\n",
        "\n",
        "  def _fit_params(self, X, y):\n",
        "    # estimate each covariance matrix\n",
        "    self.inv_covs = [inv(np.cov(X[:,y.flatten()==idx], bias=True)) \n",
        "                      for idx in range(len(self.log_a_priori))]\n",
        "    # Q5: why not just X[:,y==idx]?\n",
        "    # flatten() aplana la matriz 'y' y la convierte en un array unidimensional.\n",
        "    # en numpy cuando se trabaja con matrices de diferentes dimensiones, a veces\n",
        "    # puede haber problemas con el broadcasting, o sea la forma en la que numpy trata \n",
        "    # de combinar matrices de diferentes dimensiones para realizar operaciones. \n",
        "    # Convirtiendo la matriz 'y' en un array unidimensional se simplifican las operaciones\n",
        "    # ya que se asegura que ambas matrices de la comparación tengan dimensiones compatibles.\n",
        "\n",
        "    \n",
        "    # Q6: what does bias=True mean? why not use bias=False?\n",
        "    # significa que se utiliza la formula de covarianza con sesgo\n",
        "\n",
        "\n",
        "    self.means = [X[:,y.flatten()==idx].mean(axis=1, keepdims=True) \n",
        "                  for idx in range(len(self.log_a_priori))]\n",
        "    # Q7: what does axis=1 mean? why not axis=0 instead?\n",
        "\n",
        "    # esto significa que se calcula la media a lo largo del eje x, o sea las columnas\n",
        "    # para cada feature. Por lo que resultaen un vector columna con la media de cada feature.\n",
        "    # No se utiliza axis = 0 porque eso implicaría usar la media a lo largo del eje vertical, \n",
        "    # es decir las filas, y lo que se quiere es tener la media de cada feature por separado.\n",
        "\n",
        "  def _predict_log_conditional(self, x, class_idx):\n",
        "    # predict the log(P(x|G=class_idx)), the log of the conditional probability of x given the class\n",
        "    # this should depend on the model used\n",
        "    inv_cov = self.inv_covs[class_idx]\n",
        "    unbiased_x =  x - self.means[class_idx]\n",
        "    return 0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LDA(BaseBayesianClassifier):\n",
        "    def _fit_params(self, X, y):\n",
        "        self.means = [X[:, y.flatten() == idx].mean(axis=1, keepdims=True)\n",
        "                      for idx in range(len(self.log_a_priori))]\n",
        "        \n",
        "        cov = sum([np.cov(X[:, y.flatten() == idx], bias=True)\n",
        "                  for idx in range(len(self.log_a_priori))])\n",
        "        self.inv_cov = inv(cov)\n",
        "\n",
        "    def _predict_log_conditional(self, x, class_idx):\n",
        "        unbiased_x = x - self.means[class_idx]\n",
        "        return -0.5 * unbiased_x.T @ self.inv_cov @ unbiased_x\n",
        "\n",
        "    def _predict_one(self, x):\n",
        "        log_posteriori = [log_a_priori_i + self._predict_log_conditional(x, idx).item()\n",
        "                          for idx, log_a_priori_i in enumerate(self.log_a_priori)]\n",
        "        return np.argmax(log_posteriori)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS_zoK-gWkRf"
      },
      "source": [
        "## Código para pruebas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "m05KrhUDINVs"
      },
      "outputs": [],
      "source": [
        "# hiperparámetros\n",
        "#rng_seed = 6543\n",
        "#rng_seed = 1000\n",
        "rng_seed = 120"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hkXcoldXOqs",
        "outputId": "5e5c87e1-4251-40d1-fffc-c2da84283dba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X: (150, 4), Y:(150, 1)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "def get_iris_dataset():\n",
        "  data = load_iris()\n",
        "  X_full = data.data\n",
        "  y_full = np.array([data.target_names[y] for y in data.target.reshape(-1,1)])\n",
        "  return X_full, y_full\n",
        "\n",
        "X_full, y_full = get_iris_dataset()\n",
        "\n",
        "print(f\"X: {X_full.shape}, Y:{y_full.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAk-UQCjKecT",
        "outputId": "9e657113-b456-44e3-e997-55f151b94c10"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# peek data matrix\n",
        "X_full[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdzMURX2KVdO",
        "outputId": "46f0dd26-e088-42dc-b25d-5cccbd6b9f02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([['setosa'],\n",
              "       ['setosa'],\n",
              "       ['setosa'],\n",
              "       ['setosa'],\n",
              "       ['setosa']], dtype='<U10')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# peek target vector\n",
        "y_full[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKP_QmWCIECs",
        "outputId": "da52dc7b-6fa7-49f0-b610-b15f26f5d038"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4, 90) (1, 90) (4, 60) (1, 60)\n"
          ]
        }
      ],
      "source": [
        "# preparing data, train - test validation\n",
        "# 70-30 split\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_full, y_full, test_size=0.4, random_state=rng_seed)\n",
        "\n",
        "# traspose everything because this model format assumes column vectors\n",
        "train_x = X_train.T\n",
        "train_y = y_train.T\n",
        "test_x = X_test.T\n",
        "test_y = y_test.T\n",
        "\n",
        "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clase: setosa, Cantidad de muestras: 26\n",
            "Clase: versicolor, Cantidad de muestras: 31\n",
            "Clase: virginica, Cantidad de muestras: 33\n",
            "Proporciones de clases: [0.28888889 0.34444444 0.36666667]\n"
          ]
        }
      ],
      "source": [
        "#Análisis de datos\n",
        "unique_classes, counts = np.unique(train_y, return_counts=True)\n",
        "\n",
        "# Imprimir la cantidad de muestras para cada clase\n",
        "for unique_class, count in zip(unique_classes, counts):\n",
        "    print(f\"Clase: {unique_class}, Cantidad de muestras: {count}\")\n",
        "\n",
        "# Calcular y mostrar la proporción entre clases\n",
        "proportions = counts / counts.sum()\n",
        "print(f\"Proporciones de clases: {proportions}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### QDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dGIf2TA5SpoT"
      },
      "outputs": [],
      "source": [
        "#Entrenar el modelo con una distribución a priori normal\n",
        "qda_normal = QDA()\n",
        "qda_normal.fit(train_x, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Realizar predicciones con la distribución a priori normal\n",
        "train_y_pred_normal = qda_normal.predict(train_x)\n",
        "test_y_pred_normal = qda_normal.predict(test_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "  return (y_true == y_pred).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular la accuracy del modelo con la distribución a priori normal\n",
        "train_acc_normal = accuracy(train_y, train_y_pred_normal)\n",
        "test_acc_normal = accuracy(test_y, test_y_pred_normal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Entrenar el modelo con una distribución a priori uniforme\n",
        "qda_uniforme = QDA()\n",
        "\n",
        "# Calcular las probabilidades a priori uniformes\n",
        "n_classes = len(np.unique(y_full))\n",
        "a_priori_uniform = np.full(n_classes, 1 / n_classes)\n",
        "\n",
        "qda_uniforme.fit(train_x, train_y, a_priori=a_priori_uniform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Realizar predicciones con la distribución a priori uniforme\n",
        "train_y_pred_uniforme = qda_uniforme.predict(train_x)\n",
        "test_y_pred_uniforme = qda_uniforme.predict(test_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular la accuracy del modelo con la distribución a priori uniforme\n",
        "train_acc_uniforme = accuracy(train_y, train_y_pred_uniforme)\n",
        "test_acc_uniforme = accuracy(test_y, test_y_pred_uniforme)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QDA (normal a priori): Train (apparent) error is 0.0111 while test error is 0.0167\n",
            "QDA (uniforme a priori): Train (apparent) error is 0.0111 while test error is 0.0167\n"
          ]
        }
      ],
      "source": [
        "#Comparar accuracy de QDA con distribución a priori normal vs distribución uniforme\n",
        "print(f\"QDA (normal a priori): Train (apparent) error is {1-train_acc_normal:.4f} while test error is {1-test_acc_normal:.4f}\")\n",
        "print(f\"QDA (uniforme a priori): Train (apparent) error is {1-train_acc_uniforme:.4f} while test error is {1-test_acc_uniforme:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#LDA\n",
        "lda_normal = LDA()\n",
        "lda_normal.fit(train_x, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Realizar predicciones con la distribución a priori normal\n",
        "train_y_pred_lda_normal = lda_normal.predict(train_x)\n",
        "test_y_pred_lda_normal = lda_normal.predict(test_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular la accuracy del modelo con la distribución a priori normal\n",
        "train_acc_lda_normal = accuracy(train_y, train_y_pred_lda_normal)\n",
        "test_acc_lda_normal = accuracy(test_y, test_y_pred_lda_normal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Entrenar el modelo con una distribución a priori uniforme\n",
        "lda_uniforme = LDA()\n",
        "lda_uniforme.fit(train_x, train_y, a_priori=a_priori_uniform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Realizar predicciones con la distribución a priori uniforme\n",
        "train_y_pred_lda_uniforme = lda_uniforme.predict(train_x)\n",
        "test_y_pred_lda_uniforme = lda_uniforme.predict(test_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular la accuracy del modelo con la distribución a priori uniforme\n",
        "train_acc_lda_uniforme = accuracy(train_y, train_y_pred_lda_uniforme)\n",
        "test_acc_lda_uniforme = accuracy(test_y, test_y_pred_lda_uniforme)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Comparar accuracy de LDA con distribución a priori normal vs distribución uniforme\n",
        "print(f\"LDA (normal a priori): Train (apparent) error is {1-train_acc_lda_normal:.4f} while test error is {1-test_acc_lda_normal:.4f}\")\n",
        "print(f\"LDA (uniforme a priori): Train (apparent) error is {1-train_acc_lda_uniforme:.4f} while test error is {1-test_acc_lda_uniforme:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resultados\n",
        "\n",
        "#### ***Random seed=6543***\n",
        "\n",
        "* QDA (normal a priori): Train (apparent) error is 0.0111 while test error is 0.0167\n",
        "\n",
        "* QDA Uniforme: Train (apparent) error is 0.0222 while test error is 0.0167\n",
        "\n",
        "* LDA (normal a priori): Train (apparent) error is 0.0111 while test error is 0.0167\n",
        "\n",
        "* LDA (uniforme a priori): Train (apparent) error is 0.0222 while test error is 0.0167\n",
        "\n",
        "#### ***Random seed=1000***\n",
        "\n",
        "* QDA (normal a priori): Train (apparent) error is 0.0111 while test error is 0.0333\n",
        "\n",
        "* QDA (uniforme a priori): Train (apparent) error is 0.0111 while test error is 0.0333\n",
        "\n",
        "* LDA (normal a priori): Train (apparent) error is 0.0111 while test error is 0.0500\n",
        "\n",
        "* LDA (uniforme a priori): Train (apparent) error is 0.0111 while test error is 0.0500 \n",
        "\n",
        "\n",
        "#### ***Random seed=120***\n",
        "\n",
        "* QDA (normal a priori): Train (apparent) error is 0.0111 while test error is 0.0167\n",
        "\n",
        "* QDA (uniforme a priori): Train (apparent) error is 0.0111 while test error is 0.0167\n",
        "\n",
        "* LDA (normal a priori): Train (apparent) error is 0.0111 while test error is 0.0167\n",
        "\n",
        "* LDA (uniforme a priori): Train (apparent) error is 0.0222 while test error is 0.0167\n",
        "\n",
        "\n",
        "### Conclusiones\n",
        "\n",
        "Utilizando un random seed de 6543, el error en training es más alto en el caso de QDA con distribución uniforme a priori, esto es, el modelo está menos fiteado a los datos de training, en comparación con el modelo de QDA con distribución normal a priori. De todos modos, el test error es similar en ambos casos, por lo que ambos generalizan de manera similar. \n",
        "\n",
        "Respecto de LDA con distribución normal a priori vemos que tiene el mismo rendimiento que QDA con distribución normal para el mismo conjunto de datos, tanto en training como en test. Por lo que no podríamos decir que hay un modelo notoriamente mejor que el otro. \n",
        "\n",
        "En el caso de LDA con distribución uniforme, vemos que ocurre lo mismo, los errores de training y test son iguales que en QDA con distribución uniforme. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1Yb1V7_yXRfO"
      },
      "source": [
        "# Consigna\n",
        "\n",
        "## Implementación\n",
        "1. Entrenar un modelo QDA utilizando ahora una *a priori* uniforme ¿Se observan diferencias?¿Por qué?\n",
        "2. Implementar el modelo LDA, entrenarlo y testearlo contra los mismos sets que QDA ¿Se observan diferencias? ¿Podría decirse que alguno de los dos es notoriamente mejor que el otro?\n",
        "3. Utilizar otros 2 (dos) valores de *random seed* para obtener distintos splits de train y test, y repetir la comparación del punto anterior ¿Qué se observa?\n",
        "1. *(Opcional)* En `BaseBayesianClassifier._predict_one` se estima la posteriori de cada clase por separado, a pesar de que la cuenta es siempre la misma (cambiando valores de parámetros, pero no dimensiones). Aprovechando el *broadcasting* de NumPy, se puede reemplazar ese list comprehension por un cálculo *tensorizado* donde tanto medias como varianzas (o inversas) estén \"stackeadas\" sobre un tercer eje, permitiendo el cálculo en paralelo de todas las clases con un:\n",
        "`log_posteriori = self.tensor_log_a_priori + self._predict_log_conditionals(x)`. Implementar dicha modificación y mostrar su uso. *Ayuda: los métodos `np.stack` y la documentación del operador [`@`](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html) son de gran utilidad.*\n",
        "\n",
        "## Preguntas técnicas\n",
        "\n",
        "Responder las 7 preguntas que se encuentran distribuidas a lo largo del código.\n",
        "\n",
        "## Preguntas teóricas\n",
        "\n",
        "1. En LDA se menciona que la función a maximizar puede ser, mediante operaciones, convertida en:\n",
        "\n",
        "\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n",
        "$$\n",
        "\n",
        "<span style=\"color: blue\">\n",
        "\n",
        "Mostrar los pasos por los cuales se llega a dicha expresión. Partimos de la funcion a maximizar de QDA\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  -\\frac{1}{2}\\log |\\Sigma| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} = - \\frac{1}{2} (x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j) + C'\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  - \\frac{1}{2} [x^T \\Sigma^{-1} (x- \\mu_j) - \\mu_j^T \\Sigma^{-1} (x- \\mu_j)] + C'\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  - \\frac{1}{2} [x^T \\Sigma^{-1} x - x^T \\Sigma^{-1} \\mu_j - \\mu_j^T \\Sigma^{-1} x + \\mu_j^T \\Sigma^{-1} \\mu_j] + C'\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "Como $\\Sigma$ es una matriz cuadrada y simétrica, $\\Sigma^{-1}$ tambien simétrica y por lo tanto vale que: $x^T \\Sigma^{-1} \\mu_j = \\mu_j^T \\Sigma^{-1} x$\n",
        "\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  - \\frac{1}{2} [x^T \\Sigma^{-1} x - 2 \\mu_j^T \\Sigma^{-1} x - \\mu_j^T \\Sigma^{-1} \\mu_j] + C'\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  - \\frac{1}{2} [x^T \\Sigma^{-1} x - 2 \\mu_j^T \\Sigma^{-1} (x - \\frac{1}{2} \\mu_j)] + C'\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  - \\frac{1}{2} x^T \\Sigma^{-1} x + \\mu_j^T \\Sigma^{-1} (x - \\frac{1}{2} \\mu_j) + C'\n",
        "$$     \n",
        "\n",
        "\n",
        "\n",
        "Ahora, si comparamos dos clases, j y k haciendo el logaritmo del cociente entre ellas, se puede ver que:\n",
        "\n",
        "$$\n",
        "\\log {\\frac {P(G = j|X=x)}{P(G = k|X=x)}} = \\log {\\frac {f_j(x)}{f_k(x)}}+ \\log {\\frac {\\pi_j}{\\pi_k}} \n",
        "$$\n",
        "\n",
        "$$\n",
        "\\log {\\frac {P(G = j|X=x)}{P(G = k|X=x)}} = \\log {\\frac {\\pi_j}{\\pi_k}} - \\frac{1}{2} (\\mu_j+\\mu_k)^T \\Sigma^{-1} (\\mu_j-\\mu_k)+ x^T\\Sigma^{-1} (\\mu_j-\\mu_k)\n",
        "$$\n",
        "\n",
        "La igualdad en la matriz de covarianza entre clases ($\\Sigma_j=\\Sigma$) hace que el factor de normalización se cancele asi como también la parte cuadratica del exponente.\n",
        "\n",
        "Esta exprecion linear de probabilidades implica que el limite de decicion entre las clases $j$ y $k$ es lineal en $x$; en $p$ dimenciones un hyperplano. Esto por supuesto es verdadero para cualqueir par de clases por lo tanto todos los limites de decision son lineales. (The Elements of Statistical Learning)\n",
        "\n",
        "Por lo que al sacar la parte cuadratica, la ecuacion a maximizar de LDA queda de la forma:\n",
        "\n",
        "\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n",
        "$$\n",
        "\n",
        "</span>\n",
        "\n",
        "2. Explicar, utilizando las respectivas funciones a maximizar, por qué QDA y LDA son \"quadratic\" y \"linear\".\n",
        "\n",
        "<span style=\"color: blue\">\n",
        "\n",
        "Como se pudo observar en las ecuaciones del punto anterior de QDA y LDA:\n",
        "\n",
        "##### QDA:\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  -\\frac{1}{2}\\log |\\Sigma| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  - \\frac{1}{2} x^T \\Sigma^{-1} x + \\mu_j^T \\Sigma^{-1} (x - \\frac{1}{2} \\mu_j) + C'\n",
        "$$ \n",
        "\n",
        "##### LDA:\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} =  \\mu_j^T \\Sigma^{-1} (x- \\frac{1}{2} \\mu_j) + C'\n",
        "$$\n",
        "\n",
        "\n",
        "La función a maximizar de QDA tiene la parte cuadratica $x^T \\Sigma^{-1} x$. Esto quiere decir que los limites de decicion entre las clases se aproximan a una ecuacion cuadratica \n",
        "\n",
        "\n",
        "\n",
        "</span>\n",
        "\n",
        "\n",
        "3. La implementación de QDA estima la probabilidad condicional utilizando `0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x` que no es *exactamente* lo descrito en el apartado teórico ¿Cuáles son las diferencias y por qué son expresiones equivalentes?\n",
        "\n",
        "`0.5*np.log(det(inv_cov)) -0.5 * unbiased_x.T @ inv_cov @ unbiased_x`\n",
        "\n",
        "<span style=\"color: blue\">\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} = -\\frac{1}{2}\\log |\\Sigma_j| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\log{f_j(x)} = \\frac{1}{2}\\log |\\Sigma_j^{-1}| - \\frac{1}{2} (x-\\mu_j)^T \\Sigma_j^{-1} (x- \\mu_j) + C\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "|\\Sigma_j| = -|\\Sigma_j^{-1}|\n",
        "$$\n",
        "\n",
        "</span>\n",
        "\n",
        "El espíritu de esta componente práctica es la de establecer un mínimo de trabajo aceptable para su entrega; se invita al alumno a explorar otros aspectos que generen curiosidad, sin sentirse de ninguna manera limitado por la consigna.\n",
        "\n",
        "## Ejercicio teórico\n",
        "\n",
        "Sea una red neuronal de dos capas, la primera de 3 neuronas y la segunda de 1 con los parámetros inicializados con los siguientes valores:\n",
        "$$\n",
        "w^{(1)} = \n",
        "\\begin{pmatrix}\n",
        "0.1 & -0.5 \\\\\n",
        "-0.3 & -0.9 \\\\ \n",
        "0.8 & 0.02\n",
        "\\end{pmatrix},\n",
        "b^{(1)} = \\begin{pmatrix}\n",
        "0.1 \\\\\n",
        "0.5 \\\\\n",
        "0.8 \n",
        "\\end{pmatrix},\n",
        "w^{(2)} = \n",
        "\\begin{pmatrix}\n",
        "-0.4 & 0.2 & -0.5\n",
        "\\end{pmatrix},\n",
        "b^{(2)} = 0.7\n",
        "$$\n",
        "\n",
        "y donde cada capa calcula su salida vía\n",
        "\n",
        "$$\n",
        "y^{(i)} = \\sigma (w^{(i)} \\cdot x^{(i)}+b^{(i)})\n",
        "$$\n",
        "\n",
        "donde $\\sigma (z) = \\frac{1}{1+e^{-z}}$ es la función sigmoidea .\n",
        "\n",
        "\\\\\n",
        "Dada la observación $x=\\begin{pmatrix}\n",
        "1.8 \\\\\n",
        "-3.4\n",
        "\\end{pmatrix}$, $y=5$ y la función de costo $J(\\theta)=\\frac{1}{2}(\\hat{y}_\\theta-y)^2$, calcular las derivadas de J respecto de cada parámetro $w^{(1)}$, $w^{(2)}$, $b^{(1)}$, $b^{(2)}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ChynN-GXSL5"
      },
      "outputs": [],
      "source": [
        "º# your code should start here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
